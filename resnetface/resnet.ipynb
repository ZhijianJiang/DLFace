{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "import time, pickle, pandas\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, Conv2D, Input\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras import backend\n",
    "from keras import optimizers\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ssl\n",
    "# from sklearn.datasets import fetch_lfw_people\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "\n",
    "# class LFWLoader:\n",
    "#     \"\"\"\n",
    "#     Params:\n",
    "#         data_home: path to store LFW data\n",
    "#         target_names (ndarray): list of target names\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, data_home='~/Downloads/scikit_learn_data'):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#              data_home (str, optional): Default to '~/Downloads/scikit_learn_data'\n",
    "#         \"\"\"\n",
    "#         self.data_home = data_home\n",
    "\n",
    "#     def load_lfw_data(self,\n",
    "#                       funneled=True,\n",
    "#                       resize=1,\n",
    "#                       min_faces_per_person=2,\n",
    "#                       test_size=0.25,\n",
    "#                       random_state=42):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             funneled (boolean, optional): Default to True. Download the funneled dataset.\n",
    "#             resize (float, optional): Default to 1. Ratio to resize the face pictures.\n",
    "#             min_faces_per_person (int, optional): Default to 2. The extracted dataset will only retain pictures of people that have at least min_faces_per_person different pictures.\n",
    "#             test_size (float, int, optional): Default to 0.25. If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples.\n",
    "#             random_state (int, RandomState instance, optional): Default to 42. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator.\n",
    "#         Returns:\n",
    "#             X_train (ndarray)\n",
    "#             X_test (ndarray)\n",
    "#             y_train (ndarray): onehot label\n",
    "#             y_test (ndarray): onehot label\n",
    "#         \"\"\"\n",
    "#         # #############################################################################\n",
    "#         # Download the data, if not already on disk and load it as numpy arrays\n",
    "\n",
    "#         lfw_people = fetch_lfw_people(\n",
    "#             data_home=self.data_home,\n",
    "#             funneled=funneled,\n",
    "#             resize=resize,\n",
    "#             min_faces_per_person=min_faces_per_person,\n",
    "#             color=True\n",
    "#             )\n",
    "\n",
    "#         # introspect the images arrays to find the shapes (for plotting)\n",
    "#         n_samples, d, h, w = lfw_people.images.shape\n",
    "\n",
    "#         # for machine learning we use the 2 data directly (as relative pixel\n",
    "#         # positions info is ignored by this model)\n",
    "#         X = lfw_people.images\n",
    "#         n_features = X.shape[1]\n",
    "\n",
    "#         # the label to predict is the id of the person\n",
    "#         y = lfw_people.target\n",
    "#         y = OneHotEncoder(sparse=False).fit_transform(np.reshape(y, (len(y), 1)))\n",
    "\n",
    "#         self.target_names = lfw_people.target_names # store the list of target names\n",
    "#         n_classes = self.target_names.shape[0]\n",
    "\n",
    "#         print(\"Total dataset size:\")\n",
    "#         print(\"n_samples: %d\" % n_samples)\n",
    "#         print(\"n_features: %d\" % n_features)\n",
    "#         print(\"n_classes: %d\" % n_classes)\n",
    "#         # print(\"image size: \" + str(h) + \", \" + str(w))\n",
    "\n",
    "#         # #############################################################################\n",
    "#         # Split into a training set and a test set using a stratified k fold\n",
    "\n",
    "#         # split into a training and testing set\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(\n",
    "#             X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "#         print('Done!')\n",
    "#         return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14513 images belonging to 200 classes.\n",
      "Found 1791 images belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "nb_train = 14513\n",
    "nb_test = 1791\n",
    "img_width, img_height = 250, 250\n",
    "train_data_dir = '../CACD2000/train'\n",
    "validation_data_dir = '../CACD2000/val'\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=nb_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=nb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir='./logs/cacd2000', histogram_freq=0, write_graph=True, write_images=False)\n",
    "checkpoint = ModelCheckpoint(\"./models/cacd2000.h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from keras.models import Model\n",
    "\n",
    "# eva_data_dir = './data/c_test'\n",
    "\n",
    "# eva_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# eva_generator = eva_datagen.flow_from_directory(\n",
    "#         eva_data_dir,\n",
    "#         target_size=(img_width, img_height),\n",
    "#         batch_size=batch_size)\n",
    "\n",
    "# tf_model.evaluate_generator(eva_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_block(input_tensor, kernel_size, filters, strides=(1, 1)):\n",
    "    \"\"\"\n",
    "    Construct a residual block\n",
    "                        \n",
    "                        |------------------------>\n",
    "    1 x 1 conv, filter1      |\n",
    "                        |                        |\n",
    "    kernel_size x kernel_size conv, filter2      |\n",
    "                        |<-----------------------                         |\n",
    "    \n",
    "    Args:\n",
    "        input_tensor (Tensor): input data\n",
    "        kernel_size (int): size of 2nd layers of conv2d\n",
    "        filters (tuple of int): 1 * 3 filter value of 1st conv and 2nd conv\n",
    "    \n",
    "    Reference: \n",
    "        https://blog.waya.ai/deep-residual-learning-9610bb62c355\n",
    "        https://blog.csdn.net/wspba/article/details/56019373\n",
    "        https://blog.csdn.net/qq_25491201/article/details/78405549\n",
    "    \"\"\"\n",
    "#     shortcut = input_tensor\n",
    "    filter1, filter2 = filters\n",
    "#     x = Conv2D(filter1, strides = strides, kernel_size=(1, 1), padding='same')(input_tensor)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(filter1, kernel_size=kernel_size, strides=strides, padding='same')(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(filter2, kernel_size=kernel_size, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "#     x = Conv2D(filter3, kernel_size=(1, 1), padding='same')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "    \n",
    "    shortcut = Conv2D(filter2, kernel_size=(1, 1), strides=strides, padding='same')(input_tensor)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "    \n",
    "    x = layers.add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "    \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet(framework='tf', nb_res = (3, 0, 0, 0),input_shape=(3, 250, 250), nc=200):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    if framework == 'th':\n",
    "        # build the VGG16 network in Theano weight ordering mode\n",
    "        backend.set_image_dim_ordering('th')\n",
    "    else:\n",
    "        # build the VGG16 network in Tensorflow weight ordering mode\n",
    "        backend.set_image_dim_ordering('tf')\n",
    "    \n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    x = input_tensor\n",
    "\n",
    "    # residual units\n",
    "    \n",
    "    for i in range(nb_res[0]):\n",
    "        x = res_block(x, 3, (64, 64))\n",
    "    \n",
    "    for i in range(nb_res[1]):\n",
    "        x = res_block(x, 3, (128, 128))\n",
    "        \n",
    "    for i in range(nb_res[2]):\n",
    "        x = res_block(x, 3, (256, 256))\n",
    "       \n",
    "    for i in range(nb_res[3]):\n",
    "        x = res_block(x, 3, (512, 512))\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    x = Dense(units=nc, kernel_initializer=\"he_normal\",\n",
    "                      activation=\"softmax\")(x)\n",
    "    \n",
    "    model = Model(input_tensor, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = LFWLoader().load_lfw_data()\n",
    "X_train, y_train = train_generator.next()\n",
    "X_test, y_test = test_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size\n",
      "train data size = (14513, 250, 250, 3)\n",
      "test data size = (1791, 200)\n"
     ]
    }
   ],
   "source": [
    "print('Total dataset size')\n",
    "print('train data size = ' + str(X_train.shape))\n",
    "print('test data size = ' + str(y_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape = X_train.shape[1:]\n",
    "# model = build_resnet(input_shape=input_shape, nc=200) # tensorflow dim ordering is like this\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(x=X_train, \n",
    "#           y=y_train, \n",
    "#           batch_size=1, \n",
    "#           epochs=10, \n",
    "#           verbose=1, \n",
    "#           validation_data = (X_test, y_test),\n",
    "#           callbacks=[tensorboard_callback, checkpoint],\n",
    "#           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG + ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d_27 (ZeroPaddi (None, 252, 252, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_1 (Conv2D)             (None, 250, 250, 64)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_28 (ZeroPaddi (None, 252, 252, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv1_2 (Conv2D)             (None, 250, 250, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 125, 125, 64)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_29 (ZeroPaddi (None, 127, 127, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2_1 (Conv2D)             (None, 125, 125, 128)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_30 (ZeroPaddi (None, 127, 127, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2_2 (Conv2D)             (None, 125, 125, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 62, 62, 128)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_31 (ZeroPaddi (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv3_1 (Conv2D)             (None, 62, 62, 256)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_32 (ZeroPaddi (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv3_2 (Conv2D)             (None, 62, 62, 256)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_33 (ZeroPaddi (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv3_3 (Conv2D)             (None, 62, 62, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 31, 31, 256)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_34 (ZeroPaddi (None, 33, 33, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv4_1 (Conv2D)             (None, 31, 31, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_35 (ZeroPaddi (None, 33, 33, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv4_2 (Conv2D)             (None, 31, 31, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_36 (ZeroPaddi (None, 33, 33, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv4_3 (Conv2D)             (None, 31, 31, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_37 (ZeroPaddi (None, 17, 17, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv5_1 (Conv2D)             (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_38 (ZeroPaddi (None, 17, 17, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv5_2 (Conv2D)             (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_39 (ZeroPaddi (None, 17, 17, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv5_3 (Conv2D)             (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_vgg16(framework='tf'):\n",
    "\n",
    "    if framework == 'th':\n",
    "        # build the VGG16 network in Theano weight ordering mode\n",
    "        backend.set_image_dim_ordering('th')\n",
    "    else:\n",
    "        # build the VGG16 network in Tensorflow weight ordering mode\n",
    "        backend.set_image_dim_ordering('tf')\n",
    "        \n",
    "    model = Sequential()\n",
    "    if framework == 'th':\n",
    "        model.add(ZeroPadding2D((1, 1), input_shape=(3, img_width, img_height)))\n",
    "    else:\n",
    "        model.add(ZeroPadding2D((1, 1), input_shape=(img_width, img_height, 3)))\n",
    "        \n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', name='conv1_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', name='conv1_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', name='conv2_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', name='conv2_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', name='conv3_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', name='conv3_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', name='conv3_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', name='conv4_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', name='conv4_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', name='conv4_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', name='conv5_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', name='conv5_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', name='conv5_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    \n",
    "    return model\n",
    "\n",
    "build_vgg16().summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the model weights files.\n",
    "weights_path = '/home/zhijian_jiang95/imagenet_vgg16_fine-tuning/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "img_width = 250\n",
    "img_height = 250\n",
    "tf_model = build_vgg16('tf')\n",
    "# tf_model.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a classifier model to put on top of the convolutional model\n",
    "tf_model.add(build_resnet(input_shape=(7, 7, 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d_40 (ZeroPaddi (None, 252, 252, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_1 (Conv2D)             (None, 250, 250, 64)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_41 (ZeroPaddi (None, 252, 252, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv1_2 (Conv2D)             (None, 250, 250, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 125, 125, 64)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_42 (ZeroPaddi (None, 127, 127, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2_1 (Conv2D)             (None, 125, 125, 128)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_43 (ZeroPaddi (None, 127, 127, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2_2 (Conv2D)             (None, 125, 125, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 62, 62, 128)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_44 (ZeroPaddi (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv3_1 (Conv2D)             (None, 62, 62, 256)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_45 (ZeroPaddi (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv3_2 (Conv2D)             (None, 62, 62, 256)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_46 (ZeroPaddi (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv3_3 (Conv2D)             (None, 62, 62, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 31, 31, 256)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_47 (ZeroPaddi (None, 33, 33, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv4_1 (Conv2D)             (None, 31, 31, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_48 (ZeroPaddi (None, 33, 33, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv4_2 (Conv2D)             (None, 31, 31, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_49 (ZeroPaddi (None, 33, 33, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv4_3 (Conv2D)             (None, 31, 31, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_50 (ZeroPaddi (None, 17, 17, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv5_1 (Conv2D)             (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_51 (ZeroPaddi (None, 17, 17, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv5_2 (Conv2D)             (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_52 (ZeroPaddi (None, 17, 17, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv5_3 (Conv2D)             (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "model_3 (Model)              (None, 200)               536072    \n",
      "=================================================================\n",
      "Total params: 15,250,760\n",
      "Trainable params: 15,249,608\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set the first 25 layers (up to the last conv block)\n",
    "# # to non-trainable (weights will not be updated)\n",
    "# for layer in tf_model.layers[:25]:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# # compile the model with a SGD/momentum optimizer\n",
    "# # and a very slow learning rate.\n",
    "# tf_model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_model.load_weights('./models/cacd2000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_model.fit(x=X_train, \n",
    "#           y=y_train, \n",
    "#           batch_size=16, \n",
    "#           epochs=40, \n",
    "#           verbose=1, \n",
    "#           validation_data = (X_test, y_test),\n",
    "#           callbacks=[tensorboard_callback, checkpoint],\n",
    "#           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # change learning rate from 1e-4 to 1e-5\n",
    "# tf_model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=optimizers.SGD(lr=1e-5, momentum=0.9),\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_model.fit(x=X_train, \n",
    "#           y=y_train, \n",
    "#           batch_size=16, \n",
    "#           epochs=20, \n",
    "#           verbose=1, \n",
    "#           validation_data = (X_test, y_test),\n",
    "#           callbacks=[tensorboard_callback, checkpoint],\n",
    "#           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 14 layers into a model with 13 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e2dc8141cfdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./models/cacd2000.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m    736\u001b[0m                                                               reshape=reshape)\n\u001b[1;32m    737\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m                 \u001b[0mtopology\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   3352\u001b[0m                          \u001b[0;34m'containing '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3353\u001b[0m                          \u001b[0;34m' layers into a model with '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3354\u001b[0;31m                          str(len(filtered_layers)) + ' layers.')\n\u001b[0m\u001b[1;32m   3355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3356\u001b[0m     \u001b[0;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to load a weight file containing 14 layers into a model with 13 layers."
     ]
    }
   ],
   "source": [
    "tf_model.load_weights('./models/cacd2000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1e-5 -> 1e-6\n",
    "# tf_model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=optimizers.SGD(lr=1e-6, momentum=0.9),\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_model.fit(x=X_train, \n",
    "#           y=y_train, \n",
    "#           batch_size=16, \n",
    "#           epochs=20, \n",
    "#           verbose=1, \n",
    "#           validation_data = (X_test, y_test),\n",
    "#           callbacks=[tensorboard_callback, checkpoint],\n",
    "#           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1e-6 -> 1e-7\n",
    "# tf_model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=optimizers.SGD(lr=1e-7, momentum=0.9),\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_model.fit(x=X_train, \n",
    "#           y=y_train, \n",
    "#           batch_size=16, \n",
    "#           epochs=10, \n",
    "#           verbose=1, \n",
    "#           validation_data = (X_test, y_test),\n",
    "#           callbacks=[tensorboard_callback, checkpoint],\n",
    "#           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sgd -> adam\n",
    "# tf_model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=optimizers.Adam(lr=1e-7, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "# ,\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_model.fit(x=X_train, \n",
    "#           y=y_train, \n",
    "#           batch_size=16, \n",
    "#           epochs=10, \n",
    "#           verbose=1, \n",
    "#           validation_data = (X_test, y_test),\n",
    "#           callbacks=[tensorboard_callback, checkpoint],\n",
    "#           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreeze VGG part\n",
    "for layer in tf_model.layers[:25]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "tf_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-3, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current learning rate = 1e-05\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 676s 47ms/step - loss: 1.9190 - acc: 0.6604 - val_loss: 2.4341 - val_acc: 0.5070\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "Current learning rate = 1e-05\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 678s 47ms/step - loss: 1.8514 - acc: 0.6808 - val_loss: 2.4254 - val_acc: 0.4964\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "Current learning rate = 1e-05\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 678s 47ms/step - loss: 1.7938 - acc: 0.6982 - val_loss: 2.3850 - val_acc: 0.5109\n",
      "\n",
      "Epoch 00001: val_loss improved from 2.41463 to 2.38502, saving model to ./models/cacd2000.h5\n",
      "Current learning rate = 1e-05\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 679s 47ms/step - loss: 1.7524 - acc: 0.7132 - val_loss: 2.3865 - val_acc: 0.5081\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "Current learning rate = 1.0000000000000002e-06\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 679s 47ms/step - loss: 1.6637 - acc: 0.7360 - val_loss: 2.3495 - val_acc: 0.5248\n",
      "\n",
      "Epoch 00001: val_loss improved from 2.38502 to 2.34952, saving model to ./models/cacd2000.h5\n",
      "Current learning rate = 1.0000000000000002e-06\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 678s 47ms/step - loss: 1.6552 - acc: 0.7368 - val_loss: 2.3533 - val_acc: 0.5232\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "Current learning rate = 1.0000000000000002e-07\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 679s 47ms/step - loss: 1.6481 - acc: 0.7461 - val_loss: 2.3469 - val_acc: 0.5226\n",
      "\n",
      "Epoch 00001: val_loss improved from 2.34952 to 2.34687, saving model to ./models/cacd2000.h5\n",
      "Current learning rate = 1.0000000000000002e-07\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 679s 47ms/step - loss: 1.6372 - acc: 0.7475 - val_loss: 2.3450 - val_acc: 0.5310\n",
      "\n",
      "Epoch 00001: val_loss improved from 2.34687 to 2.34500, saving model to ./models/cacd2000.h5\n",
      "Current learning rate = 1.0000000000000002e-07\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 678s 47ms/step - loss: 1.6488 - acc: 0.7460 - val_loss: 2.3485 - val_acc: 0.5265\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "Current learning rate = 1.0000000000000004e-08\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 679s 47ms/step - loss: 1.6397 - acc: 0.7452 - val_loss: 2.3528 - val_acc: 0.5232\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "Current learning rate = 1.0000000000000005e-09\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 679s 47ms/step - loss: 1.6456 - acc: 0.7421 - val_loss: 2.3507 - val_acc: 0.5260\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "Current learning rate = 1.0000000000000006e-10\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 678s 47ms/step - loss: 1.6401 - acc: 0.7453 - val_loss: 2.3463 - val_acc: 0.5248\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "Current learning rate = 1.0000000000000006e-11\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 678s 47ms/step - loss: 1.6365 - acc: 0.7470 - val_loss: 2.3461 - val_acc: 0.5276\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "Current learning rate = 1.0000000000000006e-12\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 679s 47ms/step - loss: 1.6421 - acc: 0.7463 - val_loss: 2.3472 - val_acc: 0.5321\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "tf_model.load_weights('./models/cacd2000.h5')\n",
    "import math\n",
    "lr=1e-5\n",
    "past_loss = 2.60467 # min loss before unfreeze\n",
    "while lr > 1e-12:\n",
    "    print('Current learning rate = ' + str(lr))\n",
    "    tf_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=lr, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "    tf_model.fit(x=X_train, \n",
    "              y=y_train, \n",
    "              batch_size=16, \n",
    "              epochs=1, \n",
    "              verbose=1, \n",
    "              validation_data = (X_test, y_test),\n",
    "              callbacks=[tensorboard_callback, checkpoint],\n",
    "              shuffle=True)\n",
    "    loss, acc = tf_model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('past loss = ' + str(past_loss) + ' loss = ' + str(loss))\n",
    "    if past_loss < loss:\n",
    "        lr = lr * 0.1 # update lr if loss not improve\n",
    "    else:\n",
    "        past_loss = loss # update loss if loss improve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14513 images belonging to 200 classes.\n",
      "Found 1791 images belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "nb_train = 14513\n",
    "nb_test = 1791\n",
    "img_width, img_height = 250, 250\n",
    "train_data_dir = '../CACD2000/train'\n",
    "validation_data_dir = '../CACD2000/val'\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=nb_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=nb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = LFWLoader().load_lfw_data()\n",
    "X_train, y_train = train_generator.next()\n",
    "X_test, y_test = test_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current learning rate = 1e-05\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 703s 48ms/step - loss: 2.2036 - acc: 0.5722 - val_loss: 2.3304 - val_acc: 0.5248\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.33042, saving model to ./models/cacd2000.h5\n",
      "past loss = 2.345 loss = 2.3304226375837556\n",
      "Current learning rate = 1e-05\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 681s 47ms/step - loss: 2.0937 - acc: 0.6089 - val_loss: 2.2871 - val_acc: 0.5293\n",
      "\n",
      "Epoch 00001: val_loss improved from 2.33042 to 2.28707, saving model to ./models/cacd2000.h5\n",
      "past loss = 2.3304226375837556 loss = 2.2870698890361223\n",
      "Current learning rate = 1e-05\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 681s 47ms/step - loss: 2.0048 - acc: 0.6358 - val_loss: 2.3259 - val_acc: 0.5159\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "past loss = 2.2870698890361223 loss = 2.325877061041955\n",
      "Current learning rate = 1.0000000000000002e-06\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 681s 47ms/step - loss: 1.8927 - acc: 0.6656 - val_loss: 2.2408 - val_acc: 0.5332\n",
      "\n",
      "Epoch 00001: val_loss improved from 2.28707 to 2.24078, saving model to ./models/cacd2000.h5\n",
      "past loss = 2.2870698890361223 loss = 2.2407826259108243\n",
      "Current learning rate = 1.0000000000000002e-06\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 682s 47ms/step - loss: 1.8767 - acc: 0.6732 - val_loss: 2.2362 - val_acc: 0.5366\n",
      "\n",
      "Epoch 00001: val_loss improved from 2.24078 to 2.23625, saving model to ./models/cacd2000.h5\n",
      "past loss = 2.2407826259108243 loss = 2.2362460968282756\n",
      "Current learning rate = 1.0000000000000002e-06\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 682s 47ms/step - loss: 1.8691 - acc: 0.6725 - val_loss: 2.2326 - val_acc: 0.5388\n",
      "\n",
      "Epoch 00001: val_loss improved from 2.23625 to 2.23264, saving model to ./models/cacd2000.h5\n",
      "past loss = 2.2362460968282756 loss = 2.2326374729540945\n",
      "Current learning rate = 1.0000000000000002e-06\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 682s 47ms/step - loss: 1.8623 - acc: 0.6806 - val_loss: 2.2288 - val_acc: 0.5377\n",
      "\n",
      "Epoch 00001: val_loss improved from 2.23264 to 2.22879, saving model to ./models/cacd2000.h5\n",
      "past loss = 2.2326374729540945 loss = 2.228785571008338\n",
      "Current learning rate = 1.0000000000000002e-06\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 682s 47ms/step - loss: 1.8556 - acc: 0.6806 - val_loss: 2.2264 - val_acc: 0.5377\n",
      "\n",
      "Epoch 00001: val_loss improved from 2.22879 to 2.22645, saving model to ./models/cacd2000.h5\n",
      "past loss = 2.228785571008338 loss = 2.2264467111051847\n",
      "Current learning rate = 1.0000000000000002e-06\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 682s 47ms/step - loss: 1.8446 - acc: 0.6802 - val_loss: 2.2320 - val_acc: 0.5349\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "past loss = 2.2264467111051847 loss = 2.2320224686350523\n",
      "Current learning rate = 1.0000000000000002e-07\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 682s 47ms/step - loss: 1.8366 - acc: 0.6882 - val_loss: 2.2257 - val_acc: 0.5371\n",
      "\n",
      "Epoch 00001: val_loss improved from 2.22645 to 2.22574, saving model to ./models/cacd2000.h5\n",
      "past loss = 2.2264467111051847 loss = 2.225741433005703\n",
      "Current learning rate = 1.0000000000000002e-07\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 682s 47ms/step - loss: 1.8306 - acc: 0.6874 - val_loss: 2.2273 - val_acc: 0.5394\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "past loss = 2.225741433005703 loss = 2.2273200200563834\n",
      "Current learning rate = 1.0000000000000004e-08\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 682s 47ms/step - loss: 1.8269 - acc: 0.6899 - val_loss: 2.2265 - val_acc: 0.5360\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "past loss = 2.225741433005703 loss = 2.226488735807601\n",
      "Current learning rate = 1.0000000000000005e-09\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 682s 47ms/step - loss: 1.8337 - acc: 0.6857 - val_loss: 2.2248 - val_acc: 0.5388\n",
      "\n",
      "Epoch 00001: val_loss improved from 2.22574 to 2.22481, saving model to ./models/cacd2000.h5\n",
      "past loss = 2.225741433005703 loss = 2.224805428621025\n",
      "Current learning rate = 1.0000000000000005e-09\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 682s 47ms/step - loss: 1.8371 - acc: 0.6857 - val_loss: 2.2227 - val_acc: 0.5427\n",
      "\n",
      "Epoch 00001: val_loss improved from 2.22481 to 2.22274, saving model to ./models/cacd2000.h5\n",
      "past loss = 2.224805428621025 loss = 2.2227354112914792\n",
      "Current learning rate = 1.0000000000000005e-09\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 682s 47ms/step - loss: 1.8297 - acc: 0.6891 - val_loss: 2.2234 - val_acc: 0.5377\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "past loss = 2.2227354112914792 loss = 2.2233737856232874\n",
      "Current learning rate = 1.0000000000000006e-10\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 682s 47ms/step - loss: 1.8363 - acc: 0.6841 - val_loss: 2.2273 - val_acc: 0.5405\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "past loss = 2.2227354112914792 loss = 2.2273270803613414\n",
      "Current learning rate = 1.0000000000000006e-11\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 682s 47ms/step - loss: 1.8293 - acc: 0.6868 - val_loss: 2.2236 - val_acc: 0.5410\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "past loss = 2.2227354112914792 loss = 2.223606310564336\n",
      "Current learning rate = 1.0000000000000006e-12\n",
      "Train on 14513 samples, validate on 1791 samples\n",
      "Epoch 1/1\n",
      "14513/14513 [==============================] - 683s 47ms/step - loss: 1.8348 - acc: 0.6858 - val_loss: 2.2271 - val_acc: 0.5422\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "past loss = 2.2227354112914792 loss = 2.22711685161761\n"
     ]
    }
   ],
   "source": [
    "tf_model.load_weights('./models/cacd2000.h5')\n",
    "import math\n",
    "lr=1e-5\n",
    "past_loss = 2.34500 # min loss before unfreeze\n",
    "while lr > 1e-12:\n",
    "    print('Current learning rate = ' + str(lr))\n",
    "    tf_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=lr, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "    tf_model.fit(x=X_train, \n",
    "              y=y_train, \n",
    "              batch_size=16, \n",
    "              epochs=1, \n",
    "              verbose=1, \n",
    "              validation_data = (X_test, y_test),\n",
    "              callbacks=[tensorboard_callback, checkpoint],\n",
    "              shuffle=True)\n",
    "    loss, acc = tf_model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('past loss = ' + str(past_loss) + ' loss = ' + str(loss))\n",
    "    if past_loss < loss:\n",
    "        lr = lr * 0.1 # update lr if loss not improve\n",
    "    else:\n",
    "        past_loss = loss # update loss if loss improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
